{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76e6dc-8cb6-4b64-b7ef-7a05f9dac965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you'll need to uncomment this 2 lines if you run notebooks in vscode\n",
    "# import sys\n",
    "# sys.path.append('/home/tincho/dev/recordar_ia/')\n",
    "from src.load_data import create_data_block\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "from typing import Iterator, Union, Iterable\n",
    "from more_itertools import flatten, unique_everseen\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32d142-84fc-46ea-9bb8-fbe1c8b7e8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.warnings import PossibleUserWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=PossibleUserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86d52d-31c3-4092-974f-336faa9d26ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DATA = \"../../out_data/\"\n",
    "INPUT_DATA = \"../../input_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe209e3c-f77e-4f88-8eee-09a7d895619b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_block = create_data_block(INPUT_DATA, OUTPUT_DATA, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c669d-1705-4abd-96cc-b70504df7aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(data_block, train_size: float = 0.6, val_size: float = 0.2, test_size: float = 0.2) -> tuple:\n",
    "    \"\"\"\n",
    "    Split the data_block into train, val and test sets. The train set will be used to train the model, the val set will be used to validate the model during training and the test set will be used to test the model after training.\n",
    "\n",
    "    Args:\n",
    "        train_size (float, optional): Percentage of the data_block that will be used for training. Defaults to 0.6.\n",
    "        val_size (float, optional): Percentage of the data_block that will be used for validation. Defaults to 0.2.\n",
    "        test_size (float, optional): Percentage of the data_block that will be used for testing. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of three lists containing the train, val and test sets.\n",
    "    \"\"\"\n",
    "    # check that the sum of the sizes is 1\n",
    "    assert train_size + val_size + test_size == 1, \"The sum of the sizes must be 1\"\n",
    "\n",
    "    # get the number of samples7\n",
    "    n_samples = len(data_block)\n",
    "\n",
    "    # get the number of samples for each set\n",
    "    n_train = int(n_samples * train_size)\n",
    "    n_val = int(n_samples * val_size)\n",
    "    n_test = int(n_samples * test_size)\n",
    "\n",
    "    # get the indices for each set\n",
    "    indices = np.arange(n_samples)\n",
    "    train_indices = indices[:n_train]\n",
    "    val_indices = indices[n_train:n_train + n_val]\n",
    "    test_indices = indices[n_train + n_val:]\n",
    "\n",
    "    # get the samples for each set\n",
    "    train = [data_block[i] for i in train_indices]\n",
    "    val = [data_block[i] for i in val_indices]\n",
    "    test = [data_block[i] for i in test_indices]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = split_dataset(data_block)\n",
    "\n",
    "print(f'Train: {len(train)} \\nVal: {len(val)}\\nTest: {len(test)}\\nData_block: {len(data_block)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46798d4-72c0-40c4-93a0-57d0a6f84805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NODE_FEATURES = [\n",
    "    \"n_line\",\n",
    "    \"x_position\",\n",
    "    \"y_position\",\n",
    "    \"box_area\",\n",
    "]\n",
    "\n",
    "def get_node_features(token_box: dict):\n",
    "    node_features = [\n",
    "        v if isinstance(v, Iterable) else [v]\n",
    "        for k, v in token_box.items()\n",
    "        if k in NODE_FEATURES\n",
    "    ]\n",
    "\n",
    "    assert len(NODE_FEATURES) == len(node_features), (\n",
    "        \"mismatch in the number of node features \",\n",
    "        f\"expected => {len(NODE_FEATURES)} \",\n",
    "        f\"current  => {len(node_features)}\",\n",
    "    )\n",
    "\n",
    "    node_features = list(flatten(node_features))\n",
    "    return node_features\n",
    "\n",
    "\n",
    "def get_labels(datablock) -> Iterator[str]:\n",
    "    labels = (\n",
    "        (token[\"label\"] for token in data_item[\"token_boxes\"])\n",
    "        for data_item in datablock\n",
    "    )\n",
    "\n",
    "    labels = flatten(labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def set_label_map(datablock):\n",
    "    labels = unique_everseen(get_labels(datablock))\n",
    "    \n",
    "    label_map = {\n",
    "        label : idx\n",
    "        for idx, label in enumerate(labels)\n",
    "    }\n",
    "\n",
    "    inv_label_map = {v: k for k, v in label_map.items()}\n",
    "    \n",
    "    return label_map, inv_label_map\n",
    "\n",
    "\n",
    "def get_doc_graph(data_item, label_map) -> nx.DiGraph:\n",
    "    data_map = {\n",
    "        token_box[\"id\"]: {\n",
    "            \"node_features\": get_node_features(token_box),\n",
    "            \"label\": label_map[token_box[\"label\"]],\n",
    "        }\n",
    "        for token_box in data_item[\"token_boxes\"]\n",
    "    }\n",
    "\n",
    "    doc_graph = data_item[\"doc_graph\"]\n",
    "    node_attributes = {\n",
    "        node: {\n",
    "            \"x\": data_map[node][\"node_features\"],\n",
    "            \"y\": data_map[node][\"label\"],\n",
    "        }\n",
    "        for node in doc_graph.nodes\n",
    "    }\n",
    "\n",
    "    nx.set_node_attributes(doc_graph, node_attributes)\n",
    "    return doc_graph\n",
    "\n",
    "def get_pg_graph(doc_graph: nx.DiGraph) -> Data:\n",
    "    pg_graph = from_networkx(doc_graph)\n",
    "    pg_graph.x = pg_graph.x.float()\n",
    "    pg_graph.y = pg_graph.y.long()\n",
    "    return pg_graph\n",
    "\n",
    "def get_pg_graphs(data_block, label_map) -> list[Data]:\n",
    "    doc_graphs = [get_doc_graph(data_item,label_map) for data_item in data_block]\n",
    "    pg_graph = [get_pg_graph(doc_graph) for doc_graph in doc_graphs]\n",
    "\n",
    "    return pg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebed9aa-16a9-4d67-bc7c-f21144d7d3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_map, inv_label_map = set_label_map(train)\n",
    "\n",
    "pg_graph_train = get_pg_graphs(train, label_map)\n",
    "pg_graph_val = get_pg_graphs(val, label_map)\n",
    "pg_graph_test = get_pg_graphs(test, label_map)\n",
    "\n",
    "n_features = pg_graph_train[0].x.shape[1]\n",
    "n_classes = len(label_map)\n",
    "\n",
    "hidden_channels = 512\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "max_epochs = 5000\n",
    "\n",
    "train_monitor = \"loss\"\n",
    "es_patience = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e0028-b371-41c6-8b4a-cc96e37c704c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MONITOR_MAP = {\n",
    "    \"f1\":  {\n",
    "        \"monitor\": \"val_f1\",\n",
    "        \"mode\": \"max\",\n",
    "    },\n",
    "    \"loss\":  {\n",
    "        \"monitor\": \"val_loss\",\n",
    "        \"mode\": \"min\",\n",
    "    }\n",
    "}\n",
    "\n",
    "monitor = MONITOR_MAP[train_monitor][\"monitor\"]\n",
    "mode = MONITOR_MAP[train_monitor][\"mode\"]\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=mode,\n",
    "    min_delta=0.00,\n",
    "    patience= es_patience,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363bf0c-1377-4e71-ada4-8ae70e06f45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    pg_graph_train, batch_size=batch_size, shuffle=False, num_workers = 16\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    pg_graph_val, batch_size=batch_size, shuffle=False, num_workers = 16\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    pg_graph_test, batch_size=batch_size, shuffle=False, num_workers = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bfc4c2-be25-4e91-80af-6589e02a7f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class Model(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        hidden_channels: int,\n",
    "        n_features: int,\n",
    "        n_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.lin1 = nn.Linear(512, n_features // 2)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "        self.sage_conv1 = SAGEConv(\n",
    "            n_features,\n",
    "            hidden_channels,\n",
    "            aggr=\"mean\",\n",
    "        )\n",
    "\n",
    "        self.sage_conv2 = SAGEConv(hidden_channels, n_classes, aggr=\"mean\")\n",
    "\n",
    "        self.ce_loss = CrossEntropyLoss()\n",
    "        self.f1 = F1Score('multiclass', num_classes = n_classes, top_k=1, average=\"macro\")\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        x = self.sage_conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.sage_conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: torch.Tensor, batch_index: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        x, edge_index = (\n",
    "            batch.x,\n",
    "            batch.edge_index,\n",
    "        )\n",
    "\n",
    "        x_out = self.forward(x, edge_index)\n",
    "        loss = self.ce_loss(x_out, batch.y)\n",
    "\n",
    "        preds = x_out.argmax(dim=1)\n",
    "        self.f1(preds, batch.y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_f1\", self.f1, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: torch.Tensor):\n",
    "        \"\"\"\n",
    "        When the validation_step() is called,\n",
    "        the model has been put in eval mode\n",
    "        and PyTorch gradients have been disabled.\n",
    "        At the end of validation, the model goes back to training mode\n",
    "        and gradients are enabled.\n",
    "        \"\"\"\n",
    "\n",
    "        x, edge_index = (\n",
    "            batch.x,\n",
    "            batch.edge_index,\n",
    "        )\n",
    "\n",
    "        x_out = self.forward(x,edge_index)\n",
    "        loss = self.ce_loss(x_out, batch.y)\n",
    "\n",
    "        preds = x_out.argmax(dim=1)\n",
    "        self.f1(preds, batch.y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_f1\", self.f1, prog_bar=True)\n",
    "\n",
    "    def predict_step(\n",
    "        self, batch: torch.Tensor, batch_idx: torch.Tensor\n",
    "    ) -> list:\n",
    "\n",
    "        x, edge_index = (\n",
    "            batch.x,\n",
    "            batch.edge_index,\n",
    "        )\n",
    "        pred = self(x, edge_index)\n",
    "\n",
    "        pred = pred.softmax(dim=1)\n",
    "        confidences = pred.max(dim=1)\n",
    "        pred = pred.argmax(dim=1)\n",
    "\n",
    "        return pred, confidences\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        optimizer = Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdc121-300e-4cc3-a4f0-3d96a18065d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    hidden_channels= hidden_channels,\n",
    "    n_features= n_features,\n",
    "    n_classes= n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e66854-3c88-44bb-8cdc-7650e1f599ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs= max_epochs,\n",
    "    callbacks=[\n",
    "        early_stop_callback,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73726d-d950-4241-932f-56faa4431e3a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb1295b3-8b8f-442c-bcee-178f5234b505",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265374c-7e2b-47e8-9e67-3b5a69298b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_item_predict(data_item, pred_map: dict):\n",
    "\n",
    "    data_item = deepcopy(data_item)\n",
    "    data_item[\"token_boxes\"] = [\n",
    "        token_box | pred_map[token_box[\"id\"]]\n",
    "        for token_box in data_item[\"token_boxes\"]\n",
    "    ]\n",
    "    return data_item\n",
    "\n",
    "def predict(data_block, label_map, inv_label_map):\n",
    "    pg_graphs = get_pg_graphs(data_block, label_map)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        pg_graphs, batch_size=5, shuffle=False\n",
    "    )\n",
    "\n",
    "    pred_tuples = trainer.predict(model, loader)\n",
    "    preds = [pred[0].cpu().numpy() for pred in pred_tuples]\n",
    "    confidences = [pred[1][0].cpu().numpy() for pred in pred_tuples]\n",
    "\n",
    "    preds = np.hstack(preds)\n",
    "    confidences = np.hstack(confidences)\n",
    "\n",
    "    pred_labels = (inv_label_map[label_idx] for label_idx in preds)\n",
    "    node_ids = (\n",
    "        (token_box[\"id\"] for token_box in data[\"token_boxes\"])\n",
    "        for data in data_block\n",
    "    )\n",
    "\n",
    "    node_ids = flatten(node_ids)\n",
    "    pred_map = {\n",
    "        idx: {\"pred_label\": pred_label, \"cls_conf\": conf}\n",
    "        for idx, pred_label, conf in zip(\n",
    "            node_ids, pred_labels, confidences\n",
    "        )\n",
    "    }\n",
    "\n",
    "    data_block = [\n",
    "        data_item_predict(data_item, pred_map)\n",
    "        for data_item in data_block\n",
    "    ]\n",
    "\n",
    "    return data_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d5fd3-6a21-416d-901b-f00d6369a4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_data_block = predict(test, label_map, inv_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9ca20-6401-431a-a9ff-7f699e0273ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for data_item in predict_data_block\n",
    "    for token in data_item['token_boxes']:\n",
    "        y_true.append(token['label'])\n",
    "        y_pred.append(token['pred_label'])\n",
    "        \n",
    "len(y_true), len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7b47e-fec4-452f-9a48-bca7990e6152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = [str(i) for i in y_true]\n",
    "y_pred = [str(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb6694-cce0-4ffb-8c9c-ec42ca7da73b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label in label_map.keys():\n",
    "    print(f'{label} : true {len([i for i in y_true if i == label])} || pred {len([i for i in y_pred if i == label])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2199a6-aeb6-40e1-be8d-f494e0feda00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO de acá hacia abajo nada fue agregado en el codigo de neural_network\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def show_metrics(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    inv_label_map: dict,\n",
    "):\n",
    "\n",
    "    target_names = [str(i) for i in list(inv_label_map.values())]\n",
    "    \n",
    "    _, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        ax=ax,\n",
    "        robust=True,\n",
    "        annot=True,\n",
    "        square=False,\n",
    "        xticklabels=target_names,\n",
    "        yticklabels=target_names,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"prediction\")\n",
    "    ax.set_ylabel(\"true\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9dd73-fa45-4598-bfe1-63f35ced3775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_metrics(y_true,\n",
    "    y_pred,\n",
    "    inv_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11867a5-5c48-4b1b-aefa-742bc65f5785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
